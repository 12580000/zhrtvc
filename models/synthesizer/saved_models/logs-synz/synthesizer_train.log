
-----------------------------------------------------------------
Starting new synz training run
-----------------------------------------------------------------
[2020-12-24 11:32:20.025]  Checkpoint path: ../models/synthesizer/saved_models/logs-synz\checkpoints\model.ckpt
[2020-12-24 11:32:20.025]  Loading training data from: ../data/SV2TTS/synthesizer\train.txt
[2020-12-24 11:32:20.025]  Using model: Tacotron
[2020-12-24 11:32:20.025]  Hyperparameters:
  encoder_path: ../models/encoder/saved_models/ge2e_pretrained.pt
  cleaners: chinese_cleaners
  center: True
  tacotron_gpu_start_idx: 0
  tacotron_num_gpus: 1
  split_on_cpu: True
  inv_mel_basis: None
  mel_basis: None
  num_mels: 80
  rescale: True
  rescaling_max: 0.9
  clip_mels_length: True
  max_mel_frames: 900
  use_lws: False
  silence_threshold: 2
  n_fft: 800
  hop_size: 200
  win_size: 800
  sample_rate: 16000
  frame_shift_ms: None
  trim_fft_size: 512
  trim_hop_size: 128
  trim_top_db: 23
  signal_normalization: True
  allow_clipping_in_normalization: True
  symmetric_mels: True
  max_abs_value: 4.0
  normalize_for_wavenet: True
  clip_for_wavenet: True
  preemphasize: True
  preemphasis: 0.97
  min_level_db: -100
  ref_level_db: 20
  fmin: 55
  fmax: 7600
  power: 1.5
  griffin_lim_iters: 30
  outputs_per_step: 2
  stop_at_any: True
  embedding_dim: 256
  enc_conv_num_layers: 3
  enc_conv_kernel_size: (5,)
  enc_conv_channels: 256
  encoder_lstm_units: 128
  smoothing: False
  attention_dim: 64
  attention_filters: 32
  attention_kernel: (31,)
  cumulative_weights: True
  prenet_layers: [128, 128]
  decoder_layers: 2
  decoder_lstm_units: 512
  max_iters: 2000
  postnet_num_layers: 5
  postnet_kernel_size: (5,)
  postnet_channels: 256
  cbhg_kernels: 8
  cbhg_conv_channels: 64
  cbhg_pool_size: 2
  cbhg_projection: 128
  cbhg_projection_kernel_size: 3
  cbhg_highwaynet_layers: 4
  cbhg_highway_units: 64
  cbhg_rnn_units: 64
  mask_encoder: True
  mask_decoder: False
  cross_entropy_pos_weight: 20
  predict_linear: False
  tacotron_random_seed: 5339
  tacotron_data_random_state: 1234
  tacotron_swap_with_cpu: False
  tacotron_batch_size: 64
  tacotron_synthesis_batch_size: 128
  tacotron_test_size: None
  tacotron_test_batches: 2
  tacotron_decay_learning_rate: True
  tacotron_start_decay: 10000
  tacotron_decay_steps: 10000
  tacotron_decay_rate: 0.5
  tacotron_initial_learning_rate: 0.001
  tacotron_final_learning_rate: 1e-05
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_reg_weight: 1e-07
  tacotron_scale_regularization: False
  tacotron_zoneout_rate: 0.1
  tacotron_dropout_rate: 0.5
  tacotron_clip_gradients: True
  natural_eval: False
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_teacher_forcing_decay_steps: 280000
  tacotron_teacher_forcing_decay_alpha: 0.0
  train_with_GTA: False
  sentences: ['你好语音克隆模型。']
  speaker_embedding_size: 256
  silence_min_duration_split: 0.4
  utterance_min_duration: 1.0
[2020-12-24 11:32:20.122]  Loaded metadata for 20 examples (0.01 hours)

-----------------------------------------------------------------
Starting new synz training run
-----------------------------------------------------------------
[2020-12-24 11:35:44.038]  Checkpoint path: ../models/synthesizer/saved_models/logs-synz\checkpoints\model.ckpt
[2020-12-24 11:35:44.038]  Loading training data from: ../data/SV2TTS/synthesizer\train.txt
[2020-12-24 11:35:44.038]  Using model: Tacotron
[2020-12-24 11:35:44.038]  Hyperparameters:
  encoder_path: ../models/encoder/saved_models/ge2e_pretrained.pt
  cleaners: chinese_cleaners
  center: True
  tacotron_gpu_start_idx: 0
  tacotron_num_gpus: 1
  split_on_cpu: True
  inv_mel_basis: None
  mel_basis: None
  num_mels: 80
  rescale: True
  rescaling_max: 0.9
  clip_mels_length: True
  max_mel_frames: 900
  use_lws: False
  silence_threshold: 2
  n_fft: 800
  hop_size: 200
  win_size: 800
  sample_rate: 16000
  frame_shift_ms: None
  trim_fft_size: 512
  trim_hop_size: 128
  trim_top_db: 23
  signal_normalization: True
  allow_clipping_in_normalization: True
  symmetric_mels: True
  max_abs_value: 4.0
  normalize_for_wavenet: True
  clip_for_wavenet: True
  preemphasize: True
  preemphasis: 0.97
  min_level_db: -100
  ref_level_db: 20
  fmin: 55
  fmax: 7600
  power: 1.5
  griffin_lim_iters: 30
  outputs_per_step: 2
  stop_at_any: True
  embedding_dim: 256
  enc_conv_num_layers: 3
  enc_conv_kernel_size: (5,)
  enc_conv_channels: 256
  encoder_lstm_units: 128
  smoothing: False
  attention_dim: 64
  attention_filters: 32
  attention_kernel: (31,)
  cumulative_weights: True
  prenet_layers: [128, 128]
  decoder_layers: 2
  decoder_lstm_units: 512
  max_iters: 2000
  postnet_num_layers: 5
  postnet_kernel_size: (5,)
  postnet_channels: 256
  cbhg_kernels: 8
  cbhg_conv_channels: 64
  cbhg_pool_size: 2
  cbhg_projection: 128
  cbhg_projection_kernel_size: 3
  cbhg_highwaynet_layers: 4
  cbhg_highway_units: 64
  cbhg_rnn_units: 64
  mask_encoder: True
  mask_decoder: False
  cross_entropy_pos_weight: 20
  predict_linear: False
  tacotron_random_seed: 5339
  tacotron_data_random_state: 1234
  tacotron_swap_with_cpu: False
  tacotron_batch_size: 16
  tacotron_synthesis_batch_size: 128
  tacotron_test_size: None
  tacotron_test_batches: 2
  tacotron_decay_learning_rate: True
  tacotron_start_decay: 10000
  tacotron_decay_steps: 10000
  tacotron_decay_rate: 0.5
  tacotron_initial_learning_rate: 0.001
  tacotron_final_learning_rate: 1e-05
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_reg_weight: 1e-07
  tacotron_scale_regularization: False
  tacotron_zoneout_rate: 0.1
  tacotron_dropout_rate: 0.5
  tacotron_clip_gradients: True
  natural_eval: False
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_teacher_forcing_decay_steps: 280000
  tacotron_teacher_forcing_decay_alpha: 0.0
  train_with_GTA: False
  sentences: ['你好语音克隆模型。']
  speaker_embedding_size: 256
  silence_min_duration_split: 0.4
  utterance_min_duration: 1.0
[2020-12-24 11:35:44.042]  Loaded metadata for 20 examples (0.01 hours)
